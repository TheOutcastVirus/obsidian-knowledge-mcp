---
Created: 2025-11-11
Type: Zettel
aliases:
References:
Links:
tags:
  - MATH31AH
---
## Problem I
### 2.1.2
>![[image-73.webp]]

a. 
$\begin{bmatrix} 0 & 3 & -1 & | & 0 \\ -2 & 1 & 2 & | & 0 \\ 1 & 0 & -5 & | & 0\end{bmatrix}$
$\begin{bmatrix}1 & 0 & -5 & | & 0 \\ 0 & 3 & -1 & | & 0 \\ -2 & 1 & -2 & | & 0\end{bmatrix}$
$\begin{bmatrix}1 & 0 & -5 & | & 0 \\ 0 & 3 & -1 & | & 0 \\ -2 & 7 & 0 & | & 0\end{bmatrix}$
$\begin{bmatrix}1 & 0 & -5 & | & 0 \\ 0 & 3 & -1 & | & 0 \\ 0 & 7 & -10 & | & 0\end{bmatrix}$
$\begin{bmatrix}1 & 0 & -5 & | & 0 \\ 0 & 2.3 & 0 & | & 0 \\ 0 & 7 & -10 & | & 0\end{bmatrix}$
$\begin{bmatrix}1 & 0 & -5 & | & 0 \\ 0 & 2.3 & 0 & | & 0 \\ 0 & 0 & 10 & | & 0\end{bmatrix}$

b.
$\begin{bmatrix}2 & 3 & -1 & | & 1 \\ 0 & -2 & 1 & | & 2 \\ 1 & 0 & -2 & | & -1\end{bmatrix}$
$\begin{bmatrix}2 & 3 & -1 & | & 1 \\ 0 & -2 & 1 & | & 2 \\ 0 & -1.5 & -1.5 & | & -1.5\end{bmatrix}$
$\begin{bmatrix}2 & 3 & -1 & | & 1 \\ 0 & -2 & 1 & | & 2 \\ 0 & 0 & -2.25 & | & -3\end{bmatrix}$

### 2.13
>![[image-75.webp]]
>![[image-74.webp]]

a. 
$\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix}$
$\begin{bmatrix}1 & 2 & 3 \\ 0 & -3 & -6\end{bmatrix}$

b.
$\begin{bmatrix}1 & -1 & 1 \\ -1 & 0 & 2 \\ -1 & 1 & 1\end{bmatrix}$
$\begin{bmatrix}1 & -1 & 1 \\ -1 & 0 & 2 \\ 0 & 0 & 2\end{bmatrix}$
$\begin{bmatrix}-1 & 0 & 2 \\ 0 & -1 & 1 \\ 0 & 0 & 2\end{bmatrix}$

c. 
$\begin{bmatrix}1 & 2 & 3 & 5 \\ 2 & 3 & 0 & -1 \\ 0 & 1 & 2 & 3\end{bmatrix}$
$\begin{bmatrix}1 & 2 & 3 & 5 \\ 0 & -1 & -6 & -11 \\ 0 & 1 & 2 & 3 \\ \end{bmatrix}$
$\begin{bmatrix}1 & 2 & 3 & 5 \\ 0 & -1 & -6 & -11 \\ 0 & 0 & -4 & -8\end{bmatrix}$

d. 
$\begin{bmatrix}1 & 3 & -1 & 4 \\ 1 & 2 & 1 & 2 \\ 3 & 7 & 1 & 9\end{bmatrix}$
$\begin{bmatrix}1 & 3 & -1 & 4 \\ 0 & -1 & 2 & -2 \\ 3 & 7 & 1 & 9\end{bmatrix}$
$\begin{bmatrix}1 & 3 & -1 & 4 \\ 0 & -1 & 2 & -2 \\ 0 & -2 & -2 & -3\end{bmatrix}$
$\begin{bmatrix}1 & 3 & -1 & 4 \\ 0 & -1 & 2 & -2 \\ 0 & 0 & -6 & -7\end{bmatrix}$

e.
$\begin{bmatrix}1 & 1 & 1 & 1 \\ 2 & -3 & 3 & 3 \\ 1 & -4 & 2 & 2\end{bmatrix}$
$\begin{bmatrix}1 & 1 & 1 & 1 \\ 2 & -3 & 3 & 3 \\ 2 & -3 & 3 & 3\end{bmatrix}$
$\begin{bmatrix}1 & 1 & 1 & 1 \\ 0 & -5 & 1 & 1 \\ 0 & 0 & 0 & 0\end{bmatrix}$

## Problem II
### 1.4.2
> ![[image-76.webp]]
> ![[image-77.webp]]

a. 
$l = \sqrt{ 1^{2}+2^{2} } = \sqrt{ 5 }$

b.
$l = \sqrt{ \sqrt{ 2 }^{2} +\sqrt{ 7 }^{2}} = \sqrt{ 9 } = 3$

c. 
$l = \sqrt{ 1^{2} + (-1)^{2}+1^{2} } = \sqrt{ 3 }$

d. 
$l = \sqrt{ 1^{2}+(-2)^{2}+2^{2} } = \sqrt{ 9 } = 3$

### 1.4.5
> ![[image-78.webp]]

a. 
$\cos \theta = \dfrac{a \cdot b}{\lVert a \rVert\lVert b \rVert}$
$\cos \theta = \frac{1}{1 \cdot\sqrt{ 3 }}$
$\theta = \cos ^{-1} \left( \frac{1}{\sqrt{ 3 }} \right)$
$\theta = 0.9553\dots$

b.
$\cos \theta = \dfrac{a \cdot b}{\lVert a \rVert\lVert b \rVert}$
$a \cdot b = 0$
$\cos \theta = 0$
$\theta = \frac{\pi}{2}$

### 1.4.8
> ![[image-79.webp]]
> ![[image-80.webp]]

a. 
$\det \begin{bmatrix}1 & 2 & 3 \\ -1 & 1 & 1 \\ 2 & 2 & 2\end{bmatrix} = 1\det \begin{bmatrix}1 & 1 \\ 2 & 2\end{bmatrix} -2 \det \begin{bmatrix}-1 & 1 \\ 2 & 2\end{bmatrix} + 3\det \begin{bmatrix}-1 & 1 \\ 2 & 2\end{bmatrix}$
$=1(2-2)-2(-2-2)+3(-2-2)$
$=-4$

b.
$\det \begin{bmatrix}a & b & c \\ 0 & d & e \\ 0 & 0 & f\end{bmatrix} =a \det \begin{bmatrix}d & e \\ 0 & f\end{bmatrix} - b \det \begin{bmatrix}0 & e \\ 0 & f\end{bmatrix} + c \det \begin{bmatrix}0 & d \\ 0 & 0\end{bmatrix}$
$= a(df)-b(0)+c(0)$
$=adf$

c.
$\det \begin{bmatrix}a & b & 0 \\ c & d & 0 \\ e & f & g\end{bmatrix} = a\det \begin{bmatrix}d & 0 \\ f & g\end{bmatrix} -b\det \begin{bmatrix}c & 0 \\ e & g\end{bmatrix}$
$=a(dg)-b(cg)$
$=adg-bcg$

### 1.4.12 (a)
> ![[image-81.webp]]

We have to apply the triangle in equality to $\vec{v} = (\vec{v}+\vec{w})+(-\vec{w})$.
$\lvert \vec{v} \rvert = \lvert (\vec{v}+\vec{w}) + (-\vec{w}) \rvert \leq \lvert \vec{v}+\vec{w} \rvert + \lvert -\vec{w} \rvert$
Since $\lvert -\vec{w} \rvert = \lvert \vec{w} \rvert$, we have that  $\lvert v \rvert \leq \lvert v+w \rvert + \lvert w \rvert$. Subtract $\lvert w \rvert$ from both sides and we get that $\lvert \vec{v} \rvert - \lvert \vec{w} \rvert \leq \lvert \vec{v}+\vec{w} \rvert$, which shows the equality.

### 1.4.24
> ![[image-82.webp]]

a.
We can define $v^\perp$ as: $v^\perp := \{ s \in \mathbb{R}^n: \left< v,s \right> = 0  \}$. To show that this set is a linear subset, we need to show that the set contains the zero vector, and that it is closed under vector addition and scalar multiplication.

We know that the zero vector is in $v^\perp$, as we know that any vector dotted with the zero vector is equal to zero. $\left< \vec{0}, v \right> = 0$, for $v \in \mathbb{R}^n$. 

Select some $u,w \in v^\perp$. We need to work to show that $u+w \in v^\perp$. We have to evaluate $v \cdot(u+w)$. Since dot product is distributive over vector addition, we get that $v \cdot(u+w) = v \cdot u+ v \cdot w$. This simplifies to $0+0$, and as a result, $v \cdot(u+w)=0$ and $u + w \in v^\perp$. 

Select some $u \in v^\perp$ and $\alpha \in \mathbb{R}$. We need to show that for for all $\alpha$, that $\alpha u\in v^\perp$. We need to show that $v \cdot (\alpha u) = 0$. We can show that this is equivalent to $\alpha(v \cdot u)$, which is equal to zero. 

As a result, we know that $v^\perp$ is a linear subspace. 

### 1.4.27
> ![[image-83.webp]]
### a.
To show that $T_{\vec{a}}$ is a linear transformation, we have to show that $T_{\vec{a}}(\alpha \vec{v}+\beta \vec{w}) = \alpha T_\vec{a}(\vec{v}) + \beta T_{\vec{a}}(\vec{w})$, for some $\vec{v},\vec{w} \in \mathbb{R}^3$, $\alpha,\beta \in \mathbb{R}$. 

$T_{\vec{a}}(\vec{v}+\vec{w})=(\vec{v}+\vec{w})-2(\vec{a}\cdot(\vec{v}+\vec{w}))\vec{a}$
$=(\vec{v}+\vec{w})-2(\vec{a}\cdot \vec{v}+\vec{a}\cdot \vec{w})\vec{a}$
$=(\vec{v}+\vec{w})-2(\vec{a}\cdot \vec{v})\vec{a}-2(\vec{a}\cdot \vec{w})\vec{a}$
$=(\vec{v}-2(\vec{a}\cdot \vec{v})\vec{a})+(\vec{w}-2(\vec{a}\cdot \vec{w})\vec{a})$
$=T_{\vec{a}}(\vec{v})+T_{\vec{a}}(\vec{w})$
So $T_{\vec{a}}$ has the property of additivty.

$T_{\vec{a}}(\alpha \vec{v})=(\alpha \vec{v})-2(\vec{a}\cdot(\alpha \vec{v}))\vec{a}$
$=(\alpha \vec{v})-2\alpha(\vec{a}\cdot\vec{v})\vec{a}$
$=\alpha(\vec{v}-2(\vec{a}\cdot \vec{v})\vec{a})$
$=\alpha T_{\vec{a}}(\vec{v})$

As a result, we know that $T_{\vec{a}}(\vec{v})$ is a linear transformation from $\mathbb{R}^3 \to \mathbb{R}^3$.

## b.
$T_{\vec{a}}(\vec{a}) = \vec{a}-2(\lVert \vec{a} \rVert^{2})\vec{a}$. As we know that $\lVert \vec{a} \rVert=1$, this simplifies to $T_{\vec{a}}(\vec{a}) =\vec{a}-2\vec{a}=-\vec{a}$. 

In the case that $\vec{v}$ is orthogonal to $\vec{a}$, that would mean that $\vec{a} \cdot \vec{v}=0$. So, $T_{\vec{a}}(\vec{v}) = \vec{v}$.

This transformation is reflection across an axis that is perpendicular to $\vec{a}$.

## c.
The matrix $M$ for $T_{\vec{a}}$ is equal to: $\begin{bmatrix}T_{\vec{a}}(\vec{e}_{1}) & T_{\vec{a}}(\vec{e}_{2}) & T_{\vec{a}}(\vec{e}_{3})\end{bmatrix}$. Evaluating this, we get that $M$ is equal to:
$\begin{bmatrix}1-2a^{2} & -2ab & -2ac \\ -2ab & 1-2b^{2} & -2bc \\ -2ac & -2bc & 1-2c^{2}\end{bmatrix}$

Since this transformation is a reflection, applying the same transformation twice would have no affect. As a result, we know that $M^{2} = I$.

## Problem III
![[image-84.webp]]

### (1)
The definition of an eigenvector is a vector that when a linear transformation is applied, is only scaled by a constant fact. That constant factor is called an eigenvalue. 

### (2)
The eigenvalues of a transformation with matrix $A$ are the zeros of $\det(A-xI)$. For a matrix that is $3 \times 3$, this means that the characteristic polynomial will be a polynomial with a degree of 3. As a result, this means that there will be at least one real eigenvalue, for any given transformation. 

### (3)
Our first step here is to apply the polynomial $p(T)$ to the eigenvector $v$. As we know that $p(T)=0$, we know that $p(T)(v)=0$.

If $p(x)=a_{0}+a_{1}x+a_{2}x^{2}+\dots+a_{k}x^k$, that means that $p(T)(v)=a_{0}v+a_{1}T(v)+a_{2}T^{2}(v)+\dots+a_{k}T^k(v)$. For these cases, we can replace $T$ with $\lambda$, and factor out $v$. 

We get that $p(T)(v)=(a_{0}+a_{1}\lambda+a_{2}\lambda^{2}+\dots+a_{k}\lambda^k)v=p(\lambda)(v)$. This implies that $p(\lambda)(v)=0$. Since we know that $v$ is an eigenvector and cannot be zero, this means that $p(\lambda)=0$. By the factor theorem, we know that $(x-\lambda)$ must be one of the factors of $p(x)$.